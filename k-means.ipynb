{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises and code samples for the course **Unsupervised and Reinforcement Learning (AAI-URL)** in the Bachelor of AAI at Rosenheim University of Applied Sciences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# k-means\n",
    "\n",
    "(taken from ???; thus credits go to ???)\n",
    "\n",
    "In this tutorial, we're going to be building our own **k-means** algorithm from scratch. \n",
    "\n",
    "Recall the methodology for the k-means algorithm:\n",
    "\n",
    "1. Choose value for # of clusters K\n",
    "2. Randomly select K to start as your centroids\n",
    "3. Calculate distance of all other values to centroids\n",
    "4. Classify other values to closest centroid\n",
    "5. Take mean of each class (mean of all value by class), making that mean the new centroid\n",
    "6. Repeat steps 3-5 until optimized (centroids no longer moving)\n",
    "\n",
    "To begin, we will start with some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m      6\u001b[0m               [\u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;241m1.8\u001b[39m],\n\u001b[1;32m      7\u001b[0m               [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m8\u001b[39m ],\n\u001b[1;32m      8\u001b[0m               [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m],\n\u001b[1;32m      9\u001b[0m               [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.6\u001b[39m],\n\u001b[1;32m     10\u001b[0m               [\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m11\u001b[39m]])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.array([[1, 2],\n",
    "              [1.5, 1.8],\n",
    "              [5, 8 ],\n",
    "              [8, 8],\n",
    "              [1, 0.6],\n",
    "              [9,11]])\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], s=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be obvious where our clusters are.\n",
    "\n",
    "We're going to be choosing **k=2**. :-)\n",
    "\n",
    "\n",
    "\n",
    "# K-Means class\n",
    "\n",
    "We just set up some starting values here:\n",
    "\n",
    "- _k_   : # of clusters\n",
    "- _tol_ : is the tolerance, which will allow us to say we're optimized if the centroid is not moving more than the tolerance value\n",
    "- _n_   : value is to limit the number of cycles we're willing to run\n",
    "\n",
    "```python\n",
    "    class K_Means:\n",
    "        def __init__(self, k=2, tol=0.001, n=300):\n",
    "            self.k   = k\n",
    "            self.tol = tol\n",
    "            self.n   = n\n",
    "```\n",
    "`\n",
    "To begin, we know we just need to pass whatever data we're fitting to. We then begin an empty dictionary for our centroids.\n",
    "Next, we begin a for loop which simply assigns our starting centroids as the first two data samples in our data. If you wanted to truly select randomly the starting centroids, you could first shuffle the data, but this should be fine.\n",
    "\n",
    "```python\n",
    "    def fit(self,data):\n",
    "\n",
    "        self.centroids = {}\n",
    "\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i]\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "We start with empty classifications, and then create two dict keys (by iterating through range of self.k).\n",
    "\n",
    "Next, we need to iterate through our values, calculate distances of the values to the current centroids, and classify them as such:\n",
    "\n",
    "```python\n",
    "        for i in range(self.n):\n",
    "            self.classifications = {}\n",
    "\n",
    "            for i in range(self.k):\n",
    "                self.classifications[i] = []\n",
    "                \n",
    "            for value in data:\n",
    "                distances = [np.linalg.norm(value-self.centroids[centroid]) for centroid in self.centroids]\n",
    "                classification = distances.index(min(distances))\n",
    "                self.classifications[classification].append(value)\n",
    "```\n",
    "\n",
    "Next, we're going to need to create the new centroids, as well as measuring the movement of the centroids. If that movement is less than our tolerance (self.tol), then we're all set. Including that addition, full code up to this point:\n",
    "\n",
    "```python\n",
    "            prev_centroids = dict(self.centroids)\n",
    "\n",
    "            for classification in self.classifications:\n",
    "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)   \n",
    "```\n",
    "\n",
    "Now that we have new centroids, and knowledge of the previous centroids, we're curious if we're optimized yet. Easy enough, we will add the following to the end of the fit method:\n",
    "\n",
    "\n",
    "```python\n",
    "            optimized = True\n",
    "\n",
    "            for c in self.centroids:\n",
    "                original_centroid = prev_centroids[c]\n",
    "                current_centroid = self.centroids[c]\n",
    "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
    "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
    "                    optimized = False\n",
    "```\n",
    "\n",
    "We start off assuming we are optimized. We then take all of the centroids, and compare them to the previous centroids. If they are within our required tolerance, then we're happy. If not, then we set optimized to False and we continue in our for for i in range(self.max_iter): loop. If we are optimized?\n",
    "\n",
    "        \n",
    "The entire k-means class looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class K_Means:\n",
    "    def __init__(self, k=2, tol=0.001, n=300):\n",
    "        self.k   = k\n",
    "        self.tol = tol\n",
    "        self.n   = n\n",
    "\n",
    "    def euclidean_distance(self, X,Y):\n",
    "        return (np.sum((X-Y)**2))**0.5\n",
    "\n",
    "    def fit(self,data):\n",
    "\n",
    "        self.centroids = {}\n",
    "\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i]\n",
    "\n",
    "        for i in range(self.n):\n",
    "            self.classifications = {}\n",
    "\n",
    "            for i in range(self.k):\n",
    "                self.classifications[i] = []\n",
    "\n",
    "            for value in data:\n",
    "                # use euclidean_distance or np.linalg.norm\n",
    "                distances = [self.euclidean_distance(value, self.centroids[centroid]) for centroid in self.centroids]\n",
    "                classification = distances.index(min(distances))\n",
    "                self.classifications[classification].append(value)\n",
    "\n",
    "            prev_centroids = dict(self.centroids)\n",
    "\n",
    "            for classification in self.classifications:\n",
    "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
    "\n",
    "            optimized = True\n",
    "\n",
    "            for c in self.centroids:\n",
    "                original_centroid = prev_centroids[c]\n",
    "                current_centroid = self.centroids[c]\n",
    "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
    "                    optimized = False\n",
    "\n",
    "            if optimized:\n",
    "                break\n",
    "                \n",
    "    def plot(self):\n",
    "        colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]\n",
    "\n",
    "        #plot centroids\n",
    "        for centroid in self.centroids:\n",
    "            plt.scatter(self.centroids[centroid][0], self.centroids[centroid][1], marker=\"o\", color=\"k\", s=150, linewidths=3)\n",
    "\n",
    "        # plot smaples\n",
    "        for classification in self.classifications:\n",
    "            color = colors[classification]\n",
    "            for value in self.classifications[classification]:\n",
    "                plt.scatter(value[0], value[1], marker=\"x\", color=color, s=150, linewidths=3)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = K_Means()\n",
    "clf.fit(X)\n",
    "\n",
    "clf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we took our predictions and added them to the original dataset? That would move the centroids, and would it possibly mean new classifications for any of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.array([[1, 2],\n",
    "              [1.5, 1.8],\n",
    "              [5, 8 ],\n",
    "              [8, 8],\n",
    "              [1, 0.6],\n",
    "              [9,11],\n",
    "              [1,3],\n",
    "              [8,9],\n",
    "              [0,3],\n",
    "              [5,4],\n",
    "              [6,4],])\n",
    "\n",
    "clf = K_Means()\n",
    "clf.fit(X)\n",
    "\n",
    "clf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3b55d5e517f07ca3ce73257fc1c40bdc0d331f1850ea3324c15f26c8355ccef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
